# MemoAI 训练系统开发文档

## 项目概述

MemoAI 训练系统是一个基于 PyTorch 的深度学习对话模型训练框架，专门用于训练中文对话生成模型。该系统采用 LSTM 神经网络架构，支持字符级编码，能够处理中文文本的序列到序列学习任务。

### 核心特性
- **字符级编码**：支持完整的 Unicode 中文字符集
- **动态词汇表**：训练过程中自动构建和更新词汇表
- **模型热启动**：支持从现有模型继续训练
- **多编码支持**：自动处理不同编码格式的文本文件
- **完整的训练流程**：从数据加载到模型保存的端到端训练

## 系统架构

### 整体架构图
```
train_model.py
├── Config (配置管理)
├── CharEncoder (字符编码器)
├── LSTMDialogNet (神经网络模型)
└── ModelTrainer (训练管理器)
    ├── load_tr_corpus()    数据加载
    ├── prepare_training_data() 数据预处理
    └── train()            训练循环
```

## 核心类详解

### 1. Config 配置类

#### 功能说明
集中管理所有训练相关的配置参数，包括模型结构、训练超参数、文件路径等。

#### 配置参数
| 参数名 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| `embedding_dim` | int | 256 | 字符嵌入向量维度 |
| `hidden_size` | int | 512 | LSTM隐藏层大小 |
| `num_layers` | int | 3 | LSTM层数 |
| `batch_size` | int | 16 | 批次大小 |
| `epochs` | int | 50 | 训练轮次 |
| `learning_rate` | float | 0.0005 | 学习率 |
| `max_length` | int | 100 | 序列最大长度 |
| `model_path` | str | "model/dialog_model.pth" | 模型保存路径 |
| `vocab_path` | str | "model/vocab.json" | 词汇表保存路径 |
| `device` | str | 自动检测 | 计算设备 (cuda/cpu) |
| `dropout_rate` | float | 0.5 | Dropout比率 |

### 2. CharEncoder 字符编码器

#### 功能说明
负责字符与索引之间的双向转换，管理词汇表的构建、保存和加载。

#### 特殊字符定义
- `start_char` (\ue000): 序列开始标记
- `end_char` (\ue001): 序列结束标记  
- `pad_char` (\ue002): 填充标记

#### 核心方法

##### encode(text)
将输入文本转换为固定长度的索引序列。

**处理流程：**
1. 文本清洗：移除特殊字符和多余空白
2. 字符过滤：仅保留中文字符、字母、数字和常用标点
3. 词汇表更新：自动添加新字符到词汇表
4. 序列构建：添加开始/结束标记
5. 填充处理：填充到固定长度

**示例：**
```python
encoder = CharEncoder(config)
sequence = encoder.encode("你好世界")
# 输出: [0, 3, 4, 5, 6, 1, 2, 2, ...] (固定长度100)
```

##### decode(idx_seq)
将索引序列转换回文本，自动跳过特殊标记。

### 3. LSTMDialogNet 神经网络模型

#### 网络结构
```
输入序列 → Embedding → LSTM → LayerNorm → Linear → 输出
```

#### 层说明
1. **Embedding层**: 将字符索引映射到稠密向量空间
2. **LSTM层**: 多层LSTM处理序列依赖关系
3. **LayerNorm**: 层归一化，稳定训练过程
4. **Linear层**: 将隐藏状态映射到词汇表大小的输出

#### 输入输出
- **输入**: `(batch_size, sequence_length)` 的字符索引张量
- **输出**: `(batch_size, sequence_length, vocab_size)` 的预测概率分布

#### 模型保存与加载

##### save_model(path)
保存模型参数到指定路径，包含错误处理和日志记录。

##### load_model(config, vocab_size, model_path) [类方法]
智能加载模型，支持词汇表大小变化时的权重迁移。

**处理逻辑：**
- 检测词汇表大小变化
- 动态调整embedding层和输出层权重
- 保留共享字符的权重
- 初始化新字符的权重

### 4. ModelTrainer 训练管理器

#### 功能说明
负责整个训练流程的协调和管理，包括数据加载、预处理、模型训练和保存。

#### 训练流程

##### 1. 数据加载 (load_tr_corpus)
**功能**: 从TR文件夹加载所有.txt文本文件

**处理特点：**
- 自动检测文件编码（支持utf-8、mbcs、gbk、latin-1）
- 智能文本分割：按中文标点符号分割句子
- 句子重组：确保语义完整性
- 错误处理：跳过无法读取的文件，记录错误日志

**文件结构要求：**
```
TR/
├── file1.txt
├── file2.txt
└── ...
```

##### 2. 数据预处理 (prepare_training_data)
**功能**: 将文本转换为训练所需的张量格式

**处理步骤：**
1. 合并所有句子为长文本
2. 按固定长度切分输入-目标对
3. 字符编码转换
4. PyTorch张量化
5. 设备迁移（GPU/CPU）

##### 3. 训练循环 (train)
**完整流程：**
1. **环境检查**: 检测设备可用性
2. **数据加载**: 调用load_tr_corpus获取语料
3. **数据预处理**: 生成训练张量
4. **模型初始化**: 加载现有模型或创建新模型
5. **训练准备**: 设置损失函数、优化器、数据加载器
6. **训练循环**: 多轮次训练，每轮包含多个批次
7. **模型保存**: 保存最终模型

**训练监控：**
- 实时损失值输出（每10个批次）
- 轮次平均损失统计
- 训练日志记录到文件

## 使用方法

### 1. 环境准备
```bash
# 安装依赖
pip install torch torchvision numpy

# 确保目录结构
mkdir log model TR
```

### 2. 准备训练数据
将.txt格式的中文文本文件放入TR文件夹。

### 3. 启动训练
```bash
python train_model.py
```

### 4. 监控训练
查看`log/training.log`获取详细训练日志。

## 训练优化建议

### 1. 数据质量
- 确保文本文件编码统一（推荐UTF-8）
- 文本内容应连贯、语义完整
- 建议数据量至少10MB以上

### 2. 参数调优
- **学习率**: 根据数据量调整，小数据集建议0.001-0.0001
- **批次大小**: 根据GPU内存调整，8-32之间
- **序列长度**: 平衡上下文长度和计算效率，50-200之间
- **LSTM层数**: 3-4层通常足够

### 3. 训练监控
- 观察损失值下降趋势
- 检查是否存在过拟合（训练损失下降但验证损失上升）
- 根据损失值调整训练轮次

## 故障排除

### 常见问题

#### 1. CUDA内存不足
**症状**: RuntimeError: CUDA out of memory
**解决**: 减小batch_size或序列长度

#### 2. 词汇表加载失败
**症状**: 模型加载时报维度错误
**解决**: 删除model/vocab.json和model/dialog_model.pth重新训练

#### 3. 文本编码问题
**症状**: 中文显示乱码或读取失败
**解决**: 确保文本文件为UTF-8编码

#### 4. 训练损失不下降
**可能原因**: 
- 学习率过高
- 数据量不足
- 模型容量不足

### 日志分析
训练日志包含以下信息：
- 设备信息（GPU/CPU）
- 词汇表大小变化
- 每批次损失值
- 轮次平均损失
- 文件加载状态

## 扩展开发

### 1. 模型结构扩展
- 添加Attention机制
- 使用Transformer替代LSTM
- 增加双向LSTM

### 2. 训练策略优化
- 添加学习率调度器
- 实现早停机制
- 增加验证集评估

### 3. 数据处理增强
- 添加数据增强策略
- 实现动态序列长度
- 增加数据清洗规则

## 性能指标

### 当前配置下的预期性能
- **训练速度**: 约2-5分钟/epoch（取决于数据量和硬件）
- **内存使用**: 2-4GB GPU内存（batch_size=16）
- **最终损失**: 2.0-4.0（取决于数据质量）

### 硬件要求
- **最低配置**: 4GB内存，支持CPU训练
- **推荐配置**: 8GB+ GPU内存，NVIDIA显卡
- **理想配置**: 16GB+ GPU内存，RTX 3060以上

## 版本历史
- v1.0: 基础LSTM训练框架
- v1.1: 增加词汇表热更新
- v1.2: 增加模型热启动
- v1.3: 优化编码检测和错误处理